# Data Schemas

This document describes the schema for all `all.parquet` files in the `data/` directory.

## Citation

If you use this data, please cite:

```bibtex
@misc{weissburg2025llmsbiasedteachersevaluating,
      title={LLMs are Biased Teachers: Evaluating LLM Bias in Personalized Education},
      author={Iain Weissburg and Sathvika Anand and Sharon Levy and Haewon Jeong},
      year={2025},
      eprint={2410.14012},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2410.14012},
}
```

## Common Schema

All parquet files share a common schema generated by `scripts/dir_to_parquet.py` via the `dir_to_df()` function in `scripts/utils.py`:

| Column | Type | Description |
|--------|------|-------------|
| `topic` | VARCHAR | Subdirectory name (category/topic identifier) |
| `col_1` | VARCHAR | Content from first `.txt` file (sorted alphabetically) |
| `col_2` | VARCHAR | Content from second `.txt` file |
| `col_N` | VARCHAR | Content from Nth `.txt` file |

The number of `col_*` columns varies by dataset, corresponding to the number of `.txt` files in each subdirectory (typically representing different difficulty/complexity levels).

---

## Dataset Descriptions

### `generated/all.parquet`

LLM-generated teaching content covering diverse educational topics.

| Property | Value |
|----------|-------|
| Rows | 524 |
| Columns | `topic`, `col_1` - `col_5` |
| Levels | Beginner → Intermediate → Advanced → Expert → Master |
| Source | GPT-4o-mini generated via `scripts/gen_dataset.py` |

**Example topics**: cartooning, skateboarding-basics, machine-learning

---

### `generated-wired/all.parquet`

LLM-generated teaching content on technology/science topics (similar style to Wired articles).

| Property | Value |
|----------|-------|
| Rows | 27 |
| Columns | `topic`, `col_1` - `col_5` |
| Levels | Beginner → Intermediate → Advanced → Expert → Master |
| Source | GPT-4o-mini generated via `scripts/gen_dataset.py` |

**Example topics**: algorithms, black-holes, blockchain, crispr, quantum-computing, zk-proofs

---

### `newsinlevels/articles/all.parquet`

News articles scraped from [newsinlevels.com](https://www.newsinlevels.com), a site providing news at different reading comprehension levels.

| Property | Value |
|----------|-------|
| Rows | 3,555 |
| Columns | `topic`, `col_2` - `col_4` |
| Levels | Level 2 (easy) → Level 3 (medium) → Level 4 (hard) |
| Source | Web scraped via `data/newsinlevels/scrape.py` |

**Note**: Column numbering starts at `col_2` (no `col_1`) based on the source file naming convention.

**Example topics**: 10-million-meals, 100-years-of-the-word-robot

---

### `MATH/MATH-500/all.parquet`

Math problems from the [MATH dataset](https://huggingface.co/datasets/lighteval/math) organized by category and difficulty level.

| Property | Value |
|----------|-------|
| Rows | 3,500 |
| Columns | `topic`, `col_1` - `col_5` |
| Levels | Level 1 (easiest) → Level 5 (hardest) |
| Source | HuggingFace `lighteval/math` via `data/MATH/MATH_to_dataset.py` |

**Content format**: Each cell contains `Problem: <problem>\nSolution: <solution>`

**Example topics**: algebra-28, intermediate-algebra-47, number-theory-12

**Variants**:
- `MATH-100/all.parquet`: 700 rows (100 samples per category)
- `MATH-50/all.parquet`: 350 rows (50 samples per category)

---

### `wired/paragraphs/all.parquet`

Excerpts from Wired magazine articles at varying complexity levels.

| Property | Value |
|----------|-------|
| Rows | 27 |
| Columns | `topic`, `col_1` - `col_5` |
| Levels | Simplest → Most complex |
| Source | Curated from Wired articles, processed via `data/wired/dedialogue.py` |

**Example topics**: algorithms, black-holes, blockchain, crispr, quantum-computing

---

### `toy/all.parquet`

Minimal test dataset for development/debugging.

| Property | Value |
|----------|-------|
| Rows | 1 |
| Columns | `topic`, `col_1` - `col_3` |

---

## Usage

Query with DuckDB:

```bash
# Describe schema
duckdb -c "DESCRIBE SELECT * FROM 'data/generated/all.parquet';"

# Sample data
duckdb -c "SELECT * FROM 'data/MATH/MATH-100/all.parquet' LIMIT 5;"

# Count rows
duckdb -c "SELECT COUNT(*) FROM 'data/newsinlevels/articles/all.parquet';"
```

Load with Python:

```python
import pandas as pd

df = pd.read_parquet('data/generated/all.parquet')
```

## Creating New Datasets

To convert a directory of subdirectories containing `.txt` files into a parquet:

```bash
python scripts/dir_to_parquet.py <directory>
```

The directory structure should be:
```
directory/
├── topic-1/
│   ├── level-1.txt
│   ├── level-2.txt
│   └── level-3.txt
├── topic-2/
│   ├── level-1.txt
│   ├── level-2.txt
│   └── level-3.txt
└── all.parquet  # (generated output)
```
